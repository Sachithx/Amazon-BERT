{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b385e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ds = load_dataset(\"yassiracharki/Amazon_Reviews_for_Sentiment_Analysis_fine_grained_5_classes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebebd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'train' split to a pandas DataFrame\n",
    "df_train = ds['train'].to_pandas()\n",
    "df_test = ds['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d51522f",
   "metadata": {},
   "source": [
    "## Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc221b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset size\n",
    "print(f\"Number of samples: {len(df_train)}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(df_train.isnull().sum())\n",
    "\n",
    "# Check class distribution\n",
    "print(df_train['class_index'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbcd466",
   "metadata": {},
   "source": [
    "There are 3 million data points. Importantly there are no missing values in class or text. The 188 missing values in title section will not harmfull for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb54a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55831f98",
   "metadata": {},
   "source": [
    "The test dataset gas 650000 data points, which we will keep for the testing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f94932",
   "metadata": {},
   "source": [
    "## Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing review_text or class_index\n",
    "df_train = df_train.dropna(subset=['review_text', 'class_index'])\n",
    "\n",
    "# Optional: fill missing titles with empty string\n",
    "df_train['review_title'] = df_train['review_title'].fillna(\"\")\n",
    "df_test['review_title'] = df_test['review_title'].fillna(\"\")\n",
    "\n",
    "# Combine title and text into one column\n",
    "df_train['text'] = df_train['review_title'] + \": \" + df_train['review_text']\n",
    "\n",
    "# Basic text cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)  # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df_train['clean_text'] = df_train['text'].apply(clean_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae6bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop review_title and review_text and text\n",
    "df_train = df_train.drop(columns=['review_title', 'review_text', 'text'])\n",
    "\n",
    "# rename class_index to label and clean_text to text\n",
    "df_train = df_train.rename(columns={'class_index': 'label', 'clean_text': 'text'})\n",
    "df_test['text'] = df_test['review_text']\n",
    "df_test['label'] = df_test['class_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ed6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e49c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the length of longest review_text and review_title of train and test df\n",
    "def find_max_length(df, column):\n",
    "    max_length = df[column].apply(lambda x: len(re.findall(r'\\w+', x))).max()\n",
    "    return max_length\n",
    "max_length_train_clean_text = find_max_length(df_train, 'text')\n",
    "max_length_test_review_text = find_max_length(df_test, 'review_text')\n",
    "max_length_test_review_title = find_max_length(df_test, 'review_title')\n",
    "print(f\"Max length of clean_text in train: {max_length_train_clean_text}\")\n",
    "print(f\"Max length of review_text in test: {max_length_test_review_text}\")\n",
    "print(f\"Max length of review_title in test: {max_length_test_review_title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab24c01",
   "metadata": {},
   "source": [
    "Therefore minimum 512 length of context window is required in our transformer to process each datapoint alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad8c80fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m df_train[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].value_counts().plot(kind=\u001b[33m'\u001b[39m\u001b[33mbar\u001b[39m\u001b[33m'\u001b[39m, title=\u001b[33m'\u001b[39m\u001b[33mSentiment Distribution\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mplt\u001b[49m.show()\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train['label'].value_counts().plot(kind='bar', title='Sentiment Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc9031",
   "metadata": {},
   "source": [
    "The dataset consists of review scores, which is 5 different values form 1-5, 5 is the best. And those are well distributed closely 20% percent for each level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2823d7e",
   "metadata": {},
   "source": [
    "## Try RoBERTa based classification model to classify amazon reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4497b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3028a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = RobertaConfig()\n",
    "print(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb625ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaModel(configuration)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332a8850",
   "metadata": {},
   "source": [
    "Based on the model configs and the architecture there are few observations.\n",
    "1. the embedding lookup table is 50265x768, which means this accepts 50265 of discrete tokens and embedded them with the 768 vector.\n",
    "2. Max poisitional embedding is 512, therefore it implies that the maximum input tokens is 512 for the model, at least for the existing traiing configs it followed.\n",
    "3. 12 encoder transformer blocks which transform the input embeddings through 12 levels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e766719d",
   "metadata": {},
   "source": [
    "To input the raw data into model, we need to use a tokenizer to represent our natural language inputs by integer. In RoBERTa, they use byte pair encoding which is similar to GPT-2 tokenizer.\n",
    "So after taking the raw text, the tokenizer outputs the sequence of integers, which become thr onput to the Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5737dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "model = RobertaModel.from_pretrained(\"FacebookAI/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fbad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d94d0",
   "metadata": {},
   "source": [
    "In this assignment, we do not initiate all the parameters (weights) of the models, and train from scratch. Instead we load the pretrained weights from the trained model. -- roberta-base model.\n",
    "The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a7f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "inputs.input_ids, inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c076668",
   "metadata": {},
   "source": [
    "We convert the integer list intp pytorch tensor, as the model is built with pytorch and those accepts only tensor inputs.\n",
    "Tensors are similar to multi deimensional arrays, which are optimized for numerical methods and gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edee448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaModel.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "# n of paramas\n",
    "print(f\"Number of parameters: {model.num_parameters() / 1e6:.2f}M\")\n",
    "outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99b04eb",
   "metadata": {},
   "source": [
    "Model has around 124M parametes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7218b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state_shape = outputs.last_hidden_state.shape\n",
    "pooler_output_shape = outputs.pooler_output.shape\n",
    "print(f\"Last hidden state shape: {last_hidden_state_shape}\")\n",
    "print(f\"Pooler output shape: {pooler_output_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38a2f2",
   "metadata": {},
   "source": [
    "As we can see the models final layers, the hidden states are one hidden state representation for each input token,\n",
    "and the pooler output is the first hidden state vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703e3de",
   "metadata": {},
   "source": [
    "But, in this assignment we are planning to use differnt pooling stategies like mean/max and finally we may plan to compare the results, if possible the pooler as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb48508",
   "metadata": {},
   "source": [
    "## Building the RoBERTa Sentiment Clasifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizerFast\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Define the classifier model\n",
    "class MaxPoolRobertaClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple classifier using RoBERTa with max pooling.\n",
    "    This initiates the RoBERTa model and adds a classifier head on top.\n",
    "    The RoBERTa model is frozen to prevent training.\n",
    "    The classifier head consists of a series of linear layers with ReLU activations and dropout for regularization.\n",
    "    \n",
    "    Args:\n",
    "        num_labels (int): The number of output labels for the classification task.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels=5):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        # Freeze RoBERTa encoder\n",
    "        for param in self.roberta.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Input token IDs.\n",
    "            attention_mask (torch.Tensor): Attention mask to avoid padding tokens.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Logits for each class.\n",
    "        \"\"\"\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state  # [B, L, H]\n",
    "        \n",
    "        # Mask padding tokens for pooling\n",
    "        mask = attention_mask.unsqueeze(-1).expand(hidden_states.size())\n",
    "        masked_hidden = hidden_states.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Max pooling across token dimension\n",
    "        pooled_output = torch.max(masked_hidden, dim=1).values  # [B, H]\n",
    "        \n",
    "        # Feed to classifier head\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a458db6",
   "metadata": {},
   "source": [
    "### Building the data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4aaf3d",
   "metadata": {},
   "source": [
    "To train the model (the prediction head) we need to input the traininig data we have in an efficient way. So that we use pytorch inherent Dataset class and build out loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25728a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset for Amazon reviews\n",
    "class AmazonReviewDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['text']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Return the encoded input and label\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c04ef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader=None, epochs=5, learning_rate=2e-5):\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_accuracy = 0.0\n",
    "    \n",
    "    # Set up learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=1, verbose=True\n",
    "    )\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Progress bar for training\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            print(f\"input_ids shape: {input_ids.shape}\")\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            print(f\"attention_mask shape: {attention_mask.shape}\")\n",
    "            labels = batch['label'].to(device)\n",
    "            print(f\"labels shape: {labels.shape}\")\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Calculate loss - check for nan values and shapes\n",
    "            try:\n",
    "                # Print shape information for debugging\n",
    "                if torch.isnan(outputs).any():\n",
    "                    print(f\"Warning: outputs contain NaN values\")\n",
    "                \n",
    "                # Make sure labels are in the correct range for the model's output classes\n",
    "                if torch.max(labels) >= outputs.size(1):\n",
    "                    print(f\"Error: Labels out of range. Max label: {torch.max(labels).item()}, Output size: {outputs.size(1)}\")\n",
    "                    continue\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Check for NaN loss\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"Warning: NaN loss detected, skipping batch\")\n",
    "                    continue\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Track loss and predictions\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': train_loss / (progress_bar.n + 1)})\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "        print(f'Training Loss: {train_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.4f}')\n",
    "        \n",
    "        # Validation if provided\n",
    "        if val_loader:\n",
    "            val_loss, val_accuracy, val_report = evaluate_model(model, val_loader, criterion, device)\n",
    "            print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}')\n",
    "            print(val_report)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                torch.save(model.state_dict(), 'best_roberta_classifier.pt')\n",
    "                print(f'Model saved with accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06f5ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_loss = val_loss / len(data_loader)\n",
    "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    val_report = classification_report(all_labels, all_preds)\n",
    "    \n",
    "    return val_loss, val_accuracy, val_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50398bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict(model, text, tokenizer, device):\n",
    "    # Tokenize the text\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "    return preds.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bece39b",
   "metadata": {},
   "source": [
    "## Ctd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1638e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f91d5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1977387/2007099206.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_train = df_train.groupby('label').apply(lambda x: x.sample(frac=0.05, random_state=42)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_train = df_train.groupby('label').apply(lambda x: x.sample(frac=0.05, random_state=42)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d21eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = df_train[['text', 'label']]\n",
    "data_test = df_test[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca4a766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "987760f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AD/sachith/.conda/envs/blt_250329/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9951acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99c6b6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 100, 524, 816, 2, 1, 1], [0, 100, 524, 579, 1488, 3432, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = [\"I am playing\", \"I am sachith\"]\n",
    "tokenizer(sample_data, padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2af2b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Create a custom dataset for Amazon reviews\n",
    "class AmazonReviewDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['text']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Return the encoded input and label\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7bb3f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    print(type(p))\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecad1fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AmazonReviewDataset(data_train, tokenizer)\n",
    "val_dataset = AmazonReviewDataset(data_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a2a0feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4\n",
    "\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67e15b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AD/sachith/.conda/envs/blt_250329/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 3 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33me240203\u001b[0m (\u001b[33me240203-nanyang-technological-university-singapore\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/AD/sachith/Amazon-BERT/wandb/run-20250404_210504-1epovgky</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/e240203-nanyang-technological-university-singapore/huggingface/runs/1epovgky' target=\"_blank\">output</a></strong> to <a href='https://wandb.ai/e240203-nanyang-technological-university-singapore/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/e240203-nanyang-technological-university-singapore/huggingface' target=\"_blank\">https://wandb.ai/e240203-nanyang-technological-university-singapore/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/e240203-nanyang-technological-university-singapore/huggingface/runs/1epovgky' target=\"_blank\">https://wandb.ai/e240203-nanyang-technological-university-singapore/huggingface/runs/1epovgky</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AD/sachith/.conda/envs/blt_250329/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='7160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  30/7160 00:19 < 1:22:02, 1.45 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blt_250329",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
