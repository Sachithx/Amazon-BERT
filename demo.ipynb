{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff0fb4b",
   "metadata": {},
   "source": [
    "## Problem Definition and topic Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d1764d",
   "metadata": {},
   "source": [
    "### üìà Financial Relevance of Sentiment Classification on Amazon Reviews\n",
    "\n",
    "In this assignment, I have used Amazon large review data on their products. Those are rated from 1-5, 5 is the top review. \n",
    "However, to align this with meaningful financial application, we focused on indetifying the people's sentiment on amazon product which may lead to generated proper indicator on how the Amazon as a company would perform such as,\n",
    "\n",
    "1. The consumers sentiment (whether positive or negative or neutral) as an indicator for financial outcome of the company itself. Which indirectly helps to predict the stock trends and income growth,\n",
    "\n",
    "2. Institutional investors increasingly rely on alternative data foe the market analysis, therefore this information would be used to Feed into quantitative trading strategies to enhance the Enhance credit risk models.\n",
    "\n",
    "3. Market Timing for Consumer Goods Stocks: we can apply our model to analyze reviews for consumer discretionary companies (like Nike, Sony, or Lululemon) : Positive sentiment clusters may signal when a company is about to experience a surge in sales.\n",
    "\n",
    "4. Automating Research and Decision-Making: Reduce manual review time. Instead of reading thousands of reviews, analysts get a high-level sentiment summary. Time has direct impact on money. This solution makes impact to save huge time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d23b355",
   "metadata": {},
   "source": [
    "### üéØ Sentiment Mapping Strategy\n",
    "\n",
    "To align the model's output with real-world financial interpretation, we chose to focus on high-level sentiment categories ‚Äî representing the general public's opinion on Amazon products.\n",
    "\n",
    "#### Why Simplify to 3 Classes?\n",
    "\n",
    "Mapping the original 1‚Äì5 star review scale into three broader sentiment categories allows:\n",
    "\n",
    "- Easier interpretation by external stakeholders (investors, analysts, business strategists)\n",
    "- Improved model generalization and training efficiency\n",
    "- Better alignment with traditional sentiment analysis practices in finance\n",
    "\n",
    "#### üìä Mapping Logic\n",
    "\n",
    "To achieve this, we re-labeled the review scores as follows:\n",
    "\n",
    "- ‚≠êÔ∏è 1 & 2 ‚Üí‚ÄØ**Negative**\n",
    "- ‚≠êÔ∏è 3 ‚Üí‚ÄØ**Neutral**\n",
    "- ‚≠êÔ∏è 4 & 5 ‚Üí‚ÄØ**Positive**\n",
    "\n",
    "This transformation enables the model to focus on meaningful distinctions in sentiment and produce outputs that can be directly used in downstream financial applications (e.g., forecasting demand or stock performance based on consumer feedback).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919cc56e",
   "metadata": {},
   "source": [
    "## Analyze Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ae6389",
   "metadata": {},
   "source": [
    "### Import Libraries and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3df50f",
   "metadata": {},
   "source": [
    "We import the dataset directly from Huggingface library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1b385e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ds = load_dataset(\"yassiracharki/Amazon_Reviews_for_Sentiment_Analysis_fine_grained_5_classes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1ebebd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'train' split to a pandas DataFrame\n",
    "df_train = ds['train'].to_pandas()\n",
    "df_test = ds['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d51522f",
   "metadata": {},
   "source": [
    "### Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fc221b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_index</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>more like funchuck</td>\n",
       "      <td>Gave this to my dad for a gag gift after direc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Inspiring</td>\n",
       "      <td>I hope a lot of people hear this cd. We need m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class_index        review_title  \\\n",
       "0            3  more like funchuck   \n",
       "1            5           Inspiring   \n",
       "\n",
       "                                         review_text  \n",
       "0  Gave this to my dad for a gag gift after direc...  \n",
       "1  I hope a lot of people hear this cd. We need m...  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a5796668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_index</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>mens ultrasheer</td>\n",
       "      <td>This model may be ok for sedentary types, but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Surprisingly delightful</td>\n",
       "      <td>This is a fast read filled with unexpected hum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class_index             review_title  \\\n",
       "0            1          mens ultrasheer   \n",
       "1            4  Surprisingly delightful   \n",
       "\n",
       "                                         review_text  \n",
       "0  This model may be ok for sedentary types, but ...  \n",
       "1  This is a fast read filled with unexpected hum...  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dab7626f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 3000000\n",
      "class_index       0\n",
      "review_title    188\n",
      "review_text       0\n",
      "dtype: int64\n",
      "class_index\n",
      "3    0.2\n",
      "5    0.2\n",
      "4    0.2\n",
      "1    0.2\n",
      "2    0.2\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check dataset size\n",
    "print(f\"Number of samples: {len(df_train)}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(df_train.isnull().sum())\n",
    "\n",
    "# Check class distribution\n",
    "print(df_train['class_index'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbcd466",
   "metadata": {},
   "source": [
    "There are 3 million data points. Importantly there are no missing values in class or text. The 188 missing values in title section will not harmfull for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9bb54a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 650000 entries, 0 to 649999\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   class_index   650000 non-null  int64 \n",
      " 1   review_title  649974 non-null  object\n",
      " 2   review_text   650000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 14.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55831f98",
   "metadata": {},
   "source": [
    "The test dataset gas 650000 data points, which we will keep for the testing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f94932",
   "metadata": {},
   "source": [
    "## Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing review_text or class_index\n",
    "df_train = df_train.dropna(subset=['review_text', 'class_index'])\n",
    "\n",
    "# fill missing titles with empty string\n",
    "df_train['review_title'] = df_train['review_title'].fillna(\"\")\n",
    "df_test['review_title'] = df_test['review_title'].fillna(\"\")\n",
    "\n",
    "\n",
    "# Combine title and text into one column\n",
    "df_train['text'] = df_train['review_title'] + \": \" + df_train['review_text']\n",
    "df_test['text'] = df_test['review_title'] + \": \" + df_test['review_text']\n",
    "\n",
    "# Basic text cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)  # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df_train['clean_text'] = df_train['text'].apply(clean_text)\n",
    "df_test['clean_text'] = df_test['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "124a896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop review_title and review_text and text\n",
    "df_train = df_train.drop(columns=['review_title', 'review_text', 'text'])\n",
    "df_test = df_test.drop(columns=['review_title', 'review_text', 'text'])\n",
    "\n",
    "# rename class_index to label and clean_text to text\n",
    "df_train = df_train.rename(columns={'class_index': 'label', 'clean_text': 'text'})\n",
    "df_test = df_test.rename(columns={'class_index': 'label', 'clean_text': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0db305d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all labels to int and text to str\n",
    "df_train['label'] = df_train['label'].astype(int)\n",
    "df_test['label'] = df_test['label'].astype(int)\n",
    "\n",
    "df_train['text'] = df_train['text'].astype(str)\n",
    "df_test['text'] = df_test['text'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19009990",
   "metadata": {},
   "source": [
    "Converted all the contexts into strings and all labels into integers to improve the robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae28df",
   "metadata": {},
   "source": [
    "Investigated the missing values, and replaces missing values with empty strings to overcome errors when modeling, and performed some basic cleanings like punctuations, extra spaces and URLS removals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6105c231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of clean_text in train: 441\n"
     ]
    }
   ],
   "source": [
    "# find the length of longest review_text\n",
    "def find_max_length(df, column):\n",
    "    max_length = df[column].apply(lambda x: len(re.findall(r'\\w+', x))).max()\n",
    "    return max_length\n",
    "max_length_train_clean_text = find_max_length(df_train, 'text')\n",
    "print(f\"Max length of clean_text in train: {max_length_train_clean_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab24c01",
   "metadata": {},
   "source": [
    "Therefore minimum 512 length of context window is required in our transformer to process each datapoint alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ad8c80fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train['label'].value_counts().plot(kind='bar', title='Sentiment Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc9031",
   "metadata": {},
   "source": [
    "The dataset consists of review scores, which is 5 different values form 1-5, 5 is the best. And those are well distributed closely 20% percent for each level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce630b",
   "metadata": {},
   "source": [
    "### Transform the Dataset into sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08efe282",
   "metadata": {},
   "source": [
    "As mentioned earlier, the goal is to get highlevel understanding, therefore the 5 classes reduced to 3 classes based on sentiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cb00f0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# label 1,2 as positive, 3 as neutral, and 4,5 as negative \n",
    "\"\"\"\n",
    "    1: negative -> 0\n",
    "    2: negative -> 0\n",
    "    3: neutral -> 1\n",
    "    4: positive -> 2\n",
    "    5: positive -> 2\n",
    "\"\"\"\n",
    "# \n",
    "def map_labels(label):\n",
    "    if label in [4, 5]:\n",
    "        return 2  # positive\n",
    "    elif label == 3:\n",
    "        return 1  # neutral\n",
    "    else:\n",
    "        return 0  # negative\n",
    "df_train['label'] = df_train['label'].map(map_labels)\n",
    "df_test['label'] = df_test['label'].map(map_labels)\n",
    "# Check the new distribution\n",
    "df_train['label'].value_counts(normalize=True).plot(kind='bar', title='Sentiment Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d622ad3",
   "metadata": {},
   "source": [
    "Transforemd distribution seems unbalanced, which can lead to the model to biases results, therefore we cut down the positives and negatives by 50%\n",
    "Eventhough the 50% is large portion, still we have 1.8 million data points which is feasible amount to fine tune a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1e8906f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class distribution:\n",
      "label\n",
      "2    1200000\n",
      "0    1200000\n",
      "1     600000\n",
      "Name: count, dtype: int64\n",
      "Test class distribution:\n",
      "label\n",
      "0    260000\n",
      "2    260000\n",
      "1    130000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print n of rows in each class in train and test\n",
    "print(\"Train class distribution:\")\n",
    "print(df_train['label'].value_counts())\n",
    "print(\"Test class distribution:\")\n",
    "print(df_test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c01fea74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make label 1 randomly 600000 and -1 randomly 600000\n",
    "df_train_pos = df_train[df_train['label'] == 2].sample(n=600000, random_state=42)\n",
    "df_train_neg = df_train[df_train['label'] == 0].sample(n=600000, random_state=42)\n",
    "df_train_neutral = df_train[df_train['label'] == 1]\n",
    "df_train = pd.concat([df_train_pos, df_train_neg, df_train_neutral])\n",
    "# Check the new distribution\n",
    "df_train['label'].value_counts(normalize=True).plot(kind='bar', title='Sentiment Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0ae08ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make label 1 randomly 600000 and -1 randomly 600000\n",
    "df_test_pos = df_test[df_test['label'] == 2].sample(n=130000, random_state=42)\n",
    "df_test_neg = df_test[df_test['label'] == 0].sample(n=130000, random_state=42)\n",
    "df_test_neutral = df_test[df_test['label'] == 1]\n",
    "df_test = pd.concat([df_test_pos, df_test_neg, df_test_neutral])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9ea5c903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class distribution:\n",
      "label\n",
      "2    600000\n",
      "0    600000\n",
      "1    600000\n",
      "Name: count, dtype: int64\n",
      "Test class distribution:\n",
      "label\n",
      "2    130000\n",
      "0    130000\n",
      "1    130000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print n of rows in each class in train and test\n",
    "print(\"Train class distribution:\")\n",
    "print(df_train['label'].value_counts())\n",
    "print(\"Test class distribution:\")\n",
    "print(df_test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7508a8",
   "metadata": {},
   "source": [
    "Finally we have obtained very balanced test and train dataset for out training and validation. We save these dataset as csv file which is convinient for further steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cc75d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframes to csv\n",
    "df_train.to_csv('train.csv', index=False)\n",
    "df_test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2823d7e",
   "metadata": {},
   "source": [
    "## Try BERT based classification model to classify amazon reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d35cad0",
   "metadata": {},
   "source": [
    "### üß† Why BERT?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a3f79c",
   "metadata": {},
   "source": [
    "BERT is a transformer architecture which is trained on large text corpus, however the archotecture is special. The attention layers are not causal attention. They measure the connection within all input tokens in both direction (Bi-direction)\n",
    "Therefore, this type of architectures are used to represent the text sequences than generative tasks. And these models are much popular with fine tuning for downstream tasks. As this models have already embedded with some knowledge, so by using task specific head such as classification, we can easily obtain better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e072a55",
   "metadata": {},
   "source": [
    "In our particular case, we are classifying the text sequences into sentiments, therefore buy using 3 class classification head on top of the encoder layer would be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1f14581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "print(bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd82dd5",
   "metadata": {},
   "source": [
    "Based on the model configs and the architecture there are few observations.\n",
    "1. the embedding lookup table is 30522x768, which means this accepts 50265 of discrete tokens and embedded them with the 768 vector.\n",
    "2. Max poisitional embedding is 512, therefore it implies that the maximum input tokens is 512 for the model, at least for the existing traiing configs it followed.\n",
    "3. 12 encoder transformer blocks which transform the input embeddings through 12 levels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f42f9b",
   "metadata": {},
   "source": [
    "To input the raw data into model, we need to use a tokenizer to represent our natural language inputs by integer. In BERT, they use particular encoding which is converting the input text into integers of discrete set.\n",
    "So after taking the raw text, the tokenizer outputs the sequence of integers, which become thr input to the Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "870719f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 2088, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fd5085",
   "metadata": {},
   "source": [
    "In this assignment, we do not initiate all the parameters (weights) of the models.\n",
    "\n",
    "But we train the model on top of existing weights while adding a classification linear layer on hidden outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fcc1f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]]),\n",
       " torch.Size([1, 8]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "inputs.input_ids, inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58c9dcd",
   "metadata": {},
   "source": [
    "We convert the integer list intp pytorch tensor, as the model is built with pytorch and those accepts only tensor inputs.\n",
    "Tensors are similar to multi deimensional arrays, which are optimized for numerical methods and gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9663f441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 109482240M\n"
     ]
    }
   ],
   "source": [
    "# n of paramas\n",
    "print(f\"Number of parameters: {bert.num_parameters()}M\")\n",
    "outputs = bert(**inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fe9987",
   "metadata": {},
   "source": [
    "We can see the bert model already has 109.48 parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaaea2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last hidden state shape: torch.Size([1, 8, 768])\n",
      "Pooler output shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "last_hidden_state_shape = outputs.last_hidden_state.shape\n",
    "pooler_output_shape = outputs.pooler_output.shape\n",
    "print(f\"Last hidden state shape: {last_hidden_state_shape}\")\n",
    "print(f\"Pooler output shape: {pooler_output_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6223d5ed",
   "metadata": {},
   "source": [
    "As we can see the models final layers, the hidden states are one hidden state representation for each input token,\n",
    "and the pooler output is the first hidden state vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e50e7a",
   "metadata": {},
   "source": [
    "In this assignment we are planning to use the pooler and also differnt pooling stategies like mean/max and finally we may plan to compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc252d91",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25d3b2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertModel, BertTokenizer, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc5b59a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3144e1",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b29e07b",
   "metadata": {},
   "source": [
    "To train the model (the prediction head) we need to input the traininig data we have in an efficient way. So that we use pytorch inherent Dataset class and build out loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "deaed4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialSentimentDataset(Dataset):\n",
    "    \"\"\"Dataset for financial sentiment analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension added by tokenizer\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e912b7af",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0a7753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTSentimentClassifier(nn.Module):\n",
    "    \"\"\"BERT model for sentiment classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, bert_model_name, num_classes, dropout_prob=0.1):\n",
    "        super(BERTSentimentClassifier, self).__init__()\n",
    "        # Load pre-trained BERT model\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Get the hidden size from the BERT config\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # Create classification head\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Pass inputs through BERT\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Get the [CLS] token representation (first token)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Apply dropout\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Pass through the classifier\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03bd13de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTSentimentClassifier(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "our_model = BERTSentimentClassifier(bert_model_name=\"bert-base-uncased\", num_classes=3)\n",
    "# Check the model architecture\n",
    "print(our_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2c793d",
   "metadata": {},
   "source": [
    "  (dropout): Dropout(p=0.1, inplace=False)\n",
    "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
    "\n",
    "Now it is clear that there is another layer to classify with initialized weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "894cb5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 109484547\n"
     ]
    }
   ],
   "source": [
    "# n of params\n",
    "n_params = sum(p.numel() for p in our_model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {n_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cf4bb3",
   "metadata": {},
   "source": [
    "There is a slight increase in the number of parameters including the head compared to original model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd310665",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8877fd",
   "metadata": {},
   "source": [
    "This will load the save csv files and open as pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70346de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_path, test_path=None, test_size=0.2, random_state=42):\n",
    "    \"\"\"Load and prepare the dataset from CSV files.\"\"\"\n",
    "    \n",
    "    # Load training data\n",
    "    df_train = pd.read_csv(train_path)\n",
    "    \n",
    "    if test_path:\n",
    "        df_test = pd.read_csv(test_path)\n",
    "    else:\n",
    "        df_train, df_test = train_test_split(\n",
    "            df_train, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state,\n",
    "            stratify=df_train['label'] if 'label' in df_train.columns else None\n",
    "        )\n",
    "    \n",
    "    print(f\"Train samples: {len(df_train)}\")\n",
    "    print(f\"Test samples: {len(df_test)}\")\n",
    "    \n",
    "    # Show label distribution\n",
    "    print(\"Label distribution in training data:\")\n",
    "    print(df_train['label'].value_counts())\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020e8139",
   "metadata": {},
   "source": [
    "### Create Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cbcebb",
   "metadata": {},
   "source": [
    "This will create dataloaders for train and validation using the imported data and dataloader classes we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2755945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(train_df, val_df, tokenizer, batch_size=32, max_length=512):\n",
    "    \"\"\"Create DataLoader objects for training and validation.\"\"\"\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = FinancialSentimentDataset(\n",
    "        texts=train_df['text'].tolist(),\n",
    "        labels=train_df['label'].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    val_dataset = FinancialSentimentDataset(\n",
    "        texts=val_df['text'].tolist(),\n",
    "        labels=val_df['label'].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    \n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db520a1f",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4337b5",
   "metadata": {},
   "source": [
    "Below is the training function used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79676854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, optimizer, scheduler, device, num_epochs=3):\n",
    "    \"\"\"Train the model and evaluate on validation set.\"\"\"\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # For tracking best model\n",
    "    best_val_f1 = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n{'='*20} Epoch {epoch+1}/{num_epochs} {'='*20}\\n\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Clear previous gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            scheduler.step()  \n",
    "            \n",
    "            # Update total loss\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, accuracy, precision, recall, f1 = evaluate_model(\n",
    "            model, val_dataloader, criterion, device\n",
    "        )\n",
    "        \n",
    "        print(f\"Validation Results:\")\n",
    "        print(f\"Loss: {val_loss:.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if f1 > best_val_f1:\n",
    "            best_val_f1 = f1\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"Saved best model with F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Load best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1370908d",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3113aed7",
   "metadata": {},
   "source": [
    "This function is used to evaluate the model during the training after each epoch, as well as after training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e591db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate the model on the given dataloader.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # No gradient calculation during validation\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Update total loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='weighted'\n",
    "    )\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9967a148",
   "metadata": {},
   "source": [
    "### Predict Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d937f50",
   "metadata": {},
   "source": [
    "This function will predict the exact label for the given input, will be also used in the evaluaitons and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d6c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, tokenizer, texts, device, max_length=512, batch_size=32):\n",
    "    \"\"\"Predict sentiment for the given texts.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    # Convert single text to list\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        encodings = tokenizer(\n",
    "            batch_texts,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        input_ids = encodings['input_ids'].to(device)\n",
    "        attention_mask = encodings['attention_mask'].to(device)\n",
    "        \n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        \n",
    "        # Map predictions to sentiment labels\n",
    "        for j, text in enumerate(batch_texts):\n",
    "            sentiment = [\"Negative\", \"Neutral\", \"Positive\"][predictions[j]]\n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'sentiment': sentiment,\n",
    "                'prediction_class': int(predictions[j])\n",
    "            })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a8a6c3",
   "metadata": {},
   "source": [
    "### Training Loop Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae581107",
   "metadata": {},
   "source": [
    "Below the the example training loop which was used to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb80d7",
   "metadata": {},
   "source": [
    "### Training Setup and Resources Used\n",
    "\n",
    "Due to hardware limitations on my personal machine, I trained the model using GPU resources provided by **NTU MLDA**.\n",
    "\n",
    "To make training more efficient and scalable, I implemented several optimization strategies inspired by [Andrej Karpathy‚Äôs Reproduce GPT-2 video](https://www.youtube.com/watch?v=l8pRSuU81PU&t=12243s). I also received assistance from **Claude AI** for bug fixes and refinement.\n",
    "\n",
    "I avoid adding everything in the training loop in this notebook to make the space clear, However, the entire work and codebase is pushed to my github repository and made it private, so anyone can clone the repository and reoriduce the results.\n",
    "You can find the full training code and implementation on GitHub:\n",
    "\n",
    "üëâ [Amazon-BERT GitHub Repository](https://github.com/Sachithx/Amazon-BERT.git)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bb9a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Define hyperparameters\n",
    "    bert_model_name = \"bert-base-uncased\"\n",
    "    max_length = 512\n",
    "    batch_size = 32\n",
    "    num_epochs = 3\n",
    "    learning_rate = 2e-5\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "    \n",
    "    # Prepare data\n",
    "    train_df, val_df = prepare_data(\"train.csv\", \"test.csv\")\n",
    "    num_classes = len(train_df['label'].unique())\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataloader, val_dataloader = create_data_loaders(\n",
    "        train_df, val_df, tokenizer, batch_size, max_length\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BERTSentimentClassifier(bert_model_name, num_classes)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Set up optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Set up learning rate scheduler\n",
    "    total_steps = len(train_dataloader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),  # 10% warmup\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model = train_model(\n",
    "        model, train_dataloader, val_dataloader, optimizer, scheduler, device, num_epochs\n",
    "    )\n",
    "    \n",
    "    # Test with some examples\n",
    "    test_texts = [\n",
    "        \"The company exceeded market expectations with record profits.\",\n",
    "        \"Shares tumbled 15% following disappointing quarterly results.\",\n",
    "        \"New regulations could pose significant challenges to revenue growth.\"\n",
    "    ]\n",
    "    \n",
    "    results = predict_sentiment(model, tokenizer, test_texts, device)\n",
    "    \n",
    "    print(\"\\nSentiment predictions:\")\n",
    "    for result in results:\n",
    "        print(f\"Text: {result['text']}\")\n",
    "        print(f\"Sentiment: {result['sentiment']}\")\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64524cb9",
   "metadata": {},
   "source": [
    "## Training Stats and Evaluation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf83780",
   "metadata": {},
   "source": [
    "### Training Configuration & Hardware Setup\n",
    "\n",
    "To efficiently fine-tune the BERT model for Amazon review sentiment classification, the following training setup was used:\n",
    "\n",
    "#### Model & Training Parameters\n",
    "\n",
    "- **Model**: `bert-base-uncased`\n",
    "- **Maximum Sequence Length**: 512 tokens  \n",
    "- **Batch Size**: 64 (per GPU)\n",
    "- **Number of Epochs**: 50 (with early stopping) - I was able to train only for 3 epochs, stop as got some good accuracy and to save time\n",
    "- **Learning Rate**: 2e-5\n",
    "- **Weight Decay**: 0.01\n",
    "- **Warmup Ratio**: 10%\n",
    "- **Gradient Accumulation Steps**: 1\n",
    "- **Number of Data Loader Workers**: 2 (per GPU)\n",
    "- **Data Fraction**: 10% (subset used for quicker experimentation)\n",
    "- **Mixed Precision Training (AMP)**: Enabled \n",
    "\n",
    "#### Early Stopping Configuration\n",
    "\n",
    "- **Patience**: 5 epochs\n",
    "- **Delta Threshold**: 0.001\n",
    "- **Monitored Metric**: Accuracy\n",
    "- **Mode**: Max (higher accuracy = better)\n",
    "\n",
    "#### Hardware Used\n",
    "\n",
    "- Trained using NTU MLDA's GPU resources\n",
    "- Dual NVIDIA RTX A5000 GPUs for multi-GPU training\n",
    "\n",
    "This setup enabled faster experimentation and improved convergence thanks to multi-GPU acceleration and mixed precision optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d9a1fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "124cdcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTSentimentClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes, dropout_prob=0.1):\n",
    "        super(BERTSentimentClassifier, self).__init__()\n",
    "        config = BertConfig.from_pretrained(bert_model_name)\n",
    "        config.position_embedding_type = 'absolute'\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name, config=config)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        return self.classifier(pooled_output)\n",
    "\n",
    "\n",
    "class FinancialSentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, add_special_tokens=True, max_length=self.max_length,\n",
    "                                  return_token_type_ids=True, padding='max_length',\n",
    "                                  truncation=True, return_attention_mask=True, return_tensors='pt')\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        encoding['label'] = torch.tensor(label, dtype=torch.long)\n",
    "        return encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac5cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "model_path = './financial_sentiment_model_multi_gpu'\n",
    "num_classes = 3  \n",
    "class_names = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "def load_model(model_path, bert_model_name, num_classes, device):\n",
    "    model = BERTSentimentClassifier(bert_model_name=bert_model_name, num_classes=num_classes)\n",
    "    state_dict = torch.load(os.path.join(model_path, \"best_model_rank_0.pt\"), map_location=device)\n",
    "    if any(k.startswith('_orig_mod.') for k in state_dict.keys()):\n",
    "        state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_tokenizer(model_path, bert_model_name):\n",
    "    try:\n",
    "        return BertTokenizer.from_pretrained(model_path)\n",
    "    except:\n",
    "        return BertTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "model = load_model(model_path, bert_model_name, num_classes, device)\n",
    "tokenizer = load_tokenizer(model_path, bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3cad50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>macgyver rules i just wanna say that bad actin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>politically correct dolph i respect this movie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>good service timely shipping item was shipping...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>very interesting one note that a potential rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>good show this may be better than the studio s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      2  macgyver rules i just wanna say that bad actin...\n",
       "1      2  politically correct dolph i respect this movie...\n",
       "2      2  good service timely shipping item was shipping...\n",
       "3      2  very interesting one note that a potential rea...\n",
       "4      2  good show this may be better than the studio s..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file = 'test.csv' \n",
    "df_test = pd.read_csv(test_file)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d438b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = FinancialSentimentDataset(\n",
    "    texts=df_test['text'].tolist(),\n",
    "    labels=df_test['label'].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b58777a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc54dcb24d034736951f0184efd944fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 54\u001b[0m\n\u001b[1;32m     43\u001b[0m     avg_precision[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m average_precision_score(y_test_bin\u001b[38;5;241m.\u001b[39mravel(), all_probs\u001b[38;5;241m.\u001b[39mravel())\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m: precision, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m: recall, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m: f1,\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconf_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m: conf_matrix, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_report\u001b[39m\u001b[38;5;124m'\u001b[39m: class_report,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_precision\u001b[39m\u001b[38;5;124m'\u001b[39m: avg_precision\n\u001b[1;32m     52\u001b[0m     }\n\u001b[0;32m---> 54\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, test_dataloader, device, num_classes)\u001b[0m\n\u001b[1;32m     13\u001b[0m probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m all_preds\u001b[38;5;241m.\u001b[39mextend(\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     17\u001b[0m all_labels\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     18\u001b[0m all_probs\u001b[38;5;241m.\u001b[39mextend(probs\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_dataloader, device, num_classes):\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    class_report = classification_report(all_labels, all_preds, output_dict=True)\n",
    "\n",
    "    y_test_bin = label_binarize(all_labels, classes=list(range(num_classes)))\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], all_probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), all_probs.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    precision_curve, recall_curve, avg_precision = {}, {}, {}\n",
    "    for i in range(num_classes):\n",
    "        precision_curve[i], recall_curve[i], _ = precision_recall_curve(y_test_bin[:, i], all_probs[:, i])\n",
    "        avg_precision[i] = average_precision_score(y_test_bin[:, i], all_probs[:, i])\n",
    "    precision_curve[\"micro\"], recall_curve[\"micro\"], _ = precision_recall_curve(\n",
    "        y_test_bin.ravel(), all_probs.ravel())\n",
    "    avg_precision[\"micro\"] = average_precision_score(y_test_bin.ravel(), all_probs.ravel())\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1,\n",
    "        'conf_matrix': conf_matrix, 'class_report': class_report,\n",
    "        'all_preds': all_preds, 'all_labels': all_labels, 'all_probs': all_probs,\n",
    "        'fpr': fpr, 'tpr': tpr, 'roc_auc': roc_auc,\n",
    "        'precision_curve': precision_curve, 'recall_curve': recall_curve,\n",
    "        'avg_precision': avg_precision\n",
    "    }\n",
    "\n",
    "metrics = evaluate_model(model, test_loader, device, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df0ae83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTSentimentClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc86798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8528071d",
   "metadata": {},
   "source": [
    "### Actural and Predicted distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ada1f84",
   "metadata": {},
   "source": [
    "The following chart shows the distribution of the three sentiment classes (Negative, Neutral, Positive) after mapping the original review scores:\n",
    "\n",
    "<img src=\"/usr1/home/s124mdg54_01/Amazon-BERT/evaluation_results_si_ep_3/class_distribution.png\" alt=\"Class Distribution\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a329255b",
   "metadata": {},
   "source": [
    "It is clear that the model has learned well in terms of distribution preoperties. Using the further evaluation criterias we can explore the quality of the model in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8400aca2",
   "metadata": {},
   "source": [
    "### Quantitative Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef35603e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The model performed well across all three sentiment classes. Below are the overall metrics and the detailed classification report:\n",
    "\n",
    "\n",
    "\n",
    "| Metric     | Value   |\n",
    "|------------|---------|\n",
    "| Accuracy   | 0.7910  |\n",
    "| Precision  | 0.7889  |\n",
    "| Recall     | 0.7910  |\n",
    "| F1 Score   | 0.7890  |\n",
    "\n",
    "Per calss report\n",
    "\n",
    "\n",
    "| Class     | Precision | Recall | F1-Score | Support    |\n",
    "|-----------|-----------|--------|----------|------------|\n",
    "| Negative  | 0.7886    | 0.8575 | 0.8216   | 65,000     |\n",
    "| Neutral   | 0.7201    | 0.6570 | 0.6871   | 65,000     |\n",
    "| Positive  | 0.8582    | 0.8584 | 0.8583   | 65,000     |\n",
    "\n",
    "\n",
    "\n",
    "Specifically, The model performs best on the Positive class and shows solid generalization across the board. Neutral sentiment is slightly harder to predict, which is common due to its more ambiguous nature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dddb30a",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a04be68",
   "metadata": {},
   "source": [
    "Confusion matrix shows a geenral summary of the entire result of the model, which projects the numerical report.\n",
    "\n",
    "It is more clear now is that the neutral class is comparatively difficult to model to identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f7aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(metrics['conf_matrix'], annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de46fcbc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"/usr1/home/s124mdg54_01/Amazon-BERT/evaluation_results_si_ep_3/confusion_matrix.png\" alt=\"Class Distribution\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d46b64",
   "metadata": {},
   "source": [
    "### Confidence Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ce317",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidences = np.array([metrics['all_probs'][i, pred] for i, pred in enumerate(metrics['all_preds'])])\n",
    "correct = (metrics['all_preds'] == metrics['all_labels'])\n",
    "\n",
    "plt.hist(confidences[correct], bins=20, alpha=0.7, label='Correct', color='green')\n",
    "plt.hist(confidences[~correct], bins=20, alpha=0.7, label='Incorrect', color='red')\n",
    "plt.xlabel(\"Confidence\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Prediction Confidence Histogram\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb6b108",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"/usr1/home/s124mdg54_01/Amazon-BERT/finbert-sentiments/confidence_histogram.png\" alt=\"Class Distribution\" width=\"800\" hight=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8019220",
   "metadata": {},
   "source": [
    "Above histogram clearly shows that the probability of getting a wrong prediction is extremly low as well as their is high probability for always getting the accurate answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a936126",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160cf853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics from classification report\n",
    "metrics_to_plot = ['precision', 'recall', 'f1-score']\n",
    "class_metrics = {}\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    if str(i) in metrics['class_report']:\n",
    "        class_data = metrics['class_report'][str(i)]\n",
    "        class_metrics[class_name] = [class_data[m] for m in metrics_to_plot]\n",
    "\n",
    "# Convert to DataFrame for plotting\n",
    "df_metrics = pd.DataFrame(class_metrics, index=metrics_to_plot).T\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_metrics.plot(kind='bar')\n",
    "plt.title(\"Performance Metrics by Class\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2631f5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"/usr1/home/s124mdg54_01/Amazon-BERT/finbert-sentiments/performance_metrics.png\" alt=\"Class Distribution\" width=\"800\" hight=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918162de",
   "metadata": {},
   "source": [
    "The precision reall and f1 scores are almost near to 80% for each class and the positive class is showing promising accuracy there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b6f7fa",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_classes):\n",
    "    plt.plot(metrics['fpr'][i], metrics['tpr'][i], label=f\"Class {class_names[i]} (AUC = {metrics['roc_auc'][i]:.2f})\")\n",
    "plt.plot(metrics['fpr'][\"micro\"], metrics['tpr'][\"micro\"], 'k--', label=f\"Micro Avg (AUC = {metrics['roc_auc']['micro']:.2f})\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b17f185",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<img src=\"/usr1/home/s124mdg54_01/Amazon-BERT/finbert-sentiments/roc_curves.png\" alt=\"Class Distribution\" width=\"800\" hight=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7722914",
   "metadata": {},
   "source": [
    "In ROC Curve the diagonal line shows the random guessing, in this case 1/3 probability for each class.\n",
    "\n",
    "Howeverm in the output, it goes more closwer to upper left corner means There is very low false positive rate and high True positive rate and approaching to the ideal accurate case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3824e90",
   "metadata": {},
   "source": [
    "There is an Interesting observation!\n",
    "The Neutral class has the best upper-left curve (whether in ROC or PR), it means our model is doing surprisingly well at identifying Neutral reviews ‚Äî even though its F1 score was the lowest among the three."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba8147",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e5f6721",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bb48508",
   "metadata": {},
   "source": [
    "## Building the RoBERTa Sentiment Clasifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9250921",
   "metadata": {},
   "source": [
    "### Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizerFast\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Define the classifier model\n",
    "class MaxPoolRobertaClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple classifier using RoBERTa with max pooling.\n",
    "    This initiates the RoBERTa model and adds a classifier head on top.\n",
    "    The RoBERTa model is frozen to prevent training.\n",
    "    The classifier head consists of a series of linear layers with ReLU activations and dropout for regularization.\n",
    "    \n",
    "    Args:\n",
    "        num_labels (int): The number of output labels for the classification task.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels=5):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        # Freeze RoBERTa encoder\n",
    "        for param in self.roberta.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "        print(f\"Number of parameters: {self.num_parameters() / 1e6:.2f}M\")\n",
    "        # n of trainable params\n",
    "        self.trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Input token IDs.\n",
    "            attention_mask (torch.Tensor): Attention mask to avoid padding tokens.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Logits for each class.\n",
    "        \"\"\"\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state  # [B, L, H]\n",
    "        \n",
    "        # Mask padding tokens for pooling\n",
    "        mask = attention_mask.unsqueeze(-1).expand(hidden_states.size())\n",
    "        masked_hidden = hidden_states.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Max pooling across token dimension\n",
    "        pooled_output = torch.max(masked_hidden, dim=1).values  # [B, H]\n",
    "        \n",
    "        # Feed to classifier head\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a458db6",
   "metadata": {},
   "source": [
    "### Building the data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4aaf3d",
   "metadata": {},
   "source": [
    "To train the model (the prediction head) we need to input the traininig data we have in an efficient way. So that we use pytorch inherent Dataset class and build out loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25728a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset for Amazon reviews\n",
    "class AmazonReviewDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['text']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Return the encoded input and label\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c04ef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader=None, epochs=5, learning_rate=2e-5):\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_accuracy = 0.0\n",
    "    \n",
    "    # Set up learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=1, verbose=True\n",
    "    )\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Progress bar for training\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            print(f\"input_ids shape: {input_ids.shape}\")\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            print(f\"attention_mask shape: {attention_mask.shape}\")\n",
    "            labels = batch['label'].to(device)\n",
    "            print(f\"labels shape: {labels.shape}\")\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Calculate loss - check for nan values and shapes\n",
    "            try:\n",
    "                # Print shape information for debugging\n",
    "                if torch.isnan(outputs).any():\n",
    "                    print(f\"Warning: outputs contain NaN values\")\n",
    "                \n",
    "                # Make sure labels are in the correct range for the model's output classes\n",
    "                if torch.max(labels) >= outputs.size(1):\n",
    "                    print(f\"Error: Labels out of range. Max label: {torch.max(labels).item()}, Output size: {outputs.size(1)}\")\n",
    "                    continue\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Check for NaN loss\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"Warning: NaN loss detected, skipping batch\")\n",
    "                    continue\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Track loss and predictions\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': train_loss / (progress_bar.n + 1)})\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "        print(f'Training Loss: {train_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.4f}')\n",
    "        \n",
    "        # Validation if provided\n",
    "        if val_loader:\n",
    "            val_loss, val_accuracy, val_report = evaluate_model(model, val_loader, criterion, device)\n",
    "            print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}')\n",
    "            print(val_report)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                torch.save(model.state_dict(), 'best_roberta_classifier.pt')\n",
    "                print(f'Model saved with accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06f5ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_loss = val_loss / len(data_loader)\n",
    "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    val_report = classification_report(all_labels, all_preds)\n",
    "    \n",
    "    return val_loss, val_accuracy, val_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50398bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict(model, text, tokenizer, device):\n",
    "    # Tokenize the text\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "    return preds.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2001a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"train.csv\", index=False)\n",
    "df_test.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5aa1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: Ensure labels start from 0 and are consecutive integers\n",
    "print(\"Before label adjustment:\")\n",
    "print(f\"Train labels: {df_train['label'].unique()}\")\n",
    "print(f\"Test labels: {df_test['label'].unique()}\")\n",
    "\n",
    "# If labels are 1-indexed (1-5) instead of 0-indexed (0-4), adjust them\n",
    "if 0 not in df_train['label'].unique() and 1 in df_train['label'].unique():\n",
    "    df_train['label'] = df_train['label'] - 1\n",
    "    df_test['label'] = df_test['label'] - 1\n",
    "    print(\"After label adjustment:\")\n",
    "    print(f\"Train labels: {df_train['label'].unique()}\")\n",
    "    print(f\"Test labels: {df_test['label'].unique()}\")\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "\n",
    "# Since max text length is 441, we can use a max_length of 512 (standard for RoBERTa)\n",
    "max_length = 512\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AmazonReviewDataset(df_train, tokenizer, max_length)\n",
    "test_dataset = AmazonReviewDataset(df_test, tokenizer, max_length)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16  # Adjust based on your GPU memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac12b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = MaxPoolRobertaClassifier(num_labels=5)  # 5 classes for Amazon reviews\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    epochs=3,  # Adjust based on your needs\n",
    "    learning_rate=2e-5\n",
    ")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "test_loss, test_accuracy, test_report = evaluate_model(trained_model, test_loader, criterion, device)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}')\n",
    "print(test_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9951acdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf69fb55",
   "metadata": {},
   "source": [
    "### Results for RoBERTa based classifier with 5 classes and max pooling - disregard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1102d0a",
   "metadata": {},
   "source": [
    "<img src=\"/usr1/home/s124mdg54_01/Amazon-BERT/evaluation_results/confusion_matrix.png\" alt=\"image\" width=\"512\" height=\"512\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c86faa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b3244f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d315f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c960b4c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
