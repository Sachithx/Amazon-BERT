{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b385e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachith/miniconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ds = load_dataset(\"yassiracharki/Amazon_Reviews_for_Sentiment_Analysis_fine_grained_5_classes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ebebd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'train' split to a pandas DataFrame\n",
    "df_train = ds['train'].to_pandas()\n",
    "df_test = ds['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d51522f",
   "metadata": {},
   "source": [
    "## Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc221b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_index</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>more like funchuck</td>\n",
       "      <td>Gave this to my dad for a gag gift after direc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Inspiring</td>\n",
       "      <td>I hope a lot of people hear this cd. We need m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>The best soundtrack ever to anything.</td>\n",
       "      <td>I'm reading a lot of reviews saying that this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Chrono Cross OST</td>\n",
       "      <td>The music of Yasunori Misuda is without questi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Too good to be true</td>\n",
       "      <td>Probably the greatest soundtrack in history! U...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class_index                           review_title  \\\n",
       "0            3                     more like funchuck   \n",
       "1            5                              Inspiring   \n",
       "2            5  The best soundtrack ever to anything.   \n",
       "3            4                       Chrono Cross OST   \n",
       "4            5                    Too good to be true   \n",
       "\n",
       "                                         review_text  \n",
       "0  Gave this to my dad for a gag gift after direc...  \n",
       "1  I hope a lot of people hear this cd. We need m...  \n",
       "2  I'm reading a lot of reviews saying that this ...  \n",
       "3  The music of Yasunori Misuda is without questi...  \n",
       "4  Probably the greatest soundtrack in history! U...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dab7626f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 3000000\n",
      "class_index       0\n",
      "review_title    188\n",
      "review_text       0\n",
      "dtype: int64\n",
      "class_index\n",
      "3    0.2\n",
      "5    0.2\n",
      "4    0.2\n",
      "1    0.2\n",
      "2    0.2\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check dataset size\n",
    "print(f\"Number of samples: {len(df_train)}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(df_train.isnull().sum())\n",
    "\n",
    "# Check class distribution\n",
    "print(df_train['class_index'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbcd466",
   "metadata": {},
   "source": [
    "There are 3 million data points. Importantly there are no missing values in class or text. The 188 missing values in title section will not harmfull for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb54a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 650000 entries, 0 to 649999\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   class_index   650000 non-null  int64 \n",
      " 1   review_title  649974 non-null  object\n",
      " 2   review_text   650000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 14.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55831f98",
   "metadata": {},
   "source": [
    "The test dataset gas 650000 data points, which we will keep for the testing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f94932",
   "metadata": {},
   "source": [
    "## Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0353d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing review_text or class_index\n",
    "df_train = df_train.dropna(subset=['review_text', 'class_index'])\n",
    "\n",
    "# Optional: fill missing titles with empty string\n",
    "df_train['review_title'] = df_train['review_title'].fillna(\"\")\n",
    "df_test['review_title'] = df_test['review_title'].fillna(\"\")\n",
    "\n",
    "# Combine title and text into one column\n",
    "df_train['text'] = df_train['review_title'] + \": \" + df_train['review_text']\n",
    "\n",
    "# Basic text cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)  # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df_train['clean_text'] = df_train['text'].apply(clean_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ae6bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop review_title and review_text and text\n",
    "df_train = df_train.drop(columns=['review_title', 'review_text', 'text'])\n",
    "\n",
    "# rename class_index to label and clean_text to text\n",
    "df_train = df_train.rename(columns={'class_index': 'label', 'clean_text': 'text'})\n",
    "df_test['text'] = df_test['review_text']\n",
    "df_test['label'] = df_test['class_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "202ed6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_index</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_text</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>mens ultrasheer</td>\n",
       "      <td>This model may be ok for sedentary types, but ...</td>\n",
       "      <td>This model may be ok for sedentary types, but ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Surprisingly delightful</td>\n",
       "      <td>This is a fast read filled with unexpected hum...</td>\n",
       "      <td>This is a fast read filled with unexpected hum...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Works, but not as advertised</td>\n",
       "      <td>I bought one of these chargers..the instructio...</td>\n",
       "      <td>I bought one of these chargers..the instructio...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Oh dear</td>\n",
       "      <td>I was excited to find a book ostensibly about ...</td>\n",
       "      <td>I was excited to find a book ostensibly about ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Incorrect disc!</td>\n",
       "      <td>I am a big JVC fan, but I do not like this mod...</td>\n",
       "      <td>I am a big JVC fan, but I do not like this mod...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class_index                  review_title  \\\n",
       "0            1               mens ultrasheer   \n",
       "1            4       Surprisingly delightful   \n",
       "2            2  Works, but not as advertised   \n",
       "3            2                       Oh dear   \n",
       "4            2               Incorrect disc!   \n",
       "\n",
       "                                         review_text  \\\n",
       "0  This model may be ok for sedentary types, but ...   \n",
       "1  This is a fast read filled with unexpected hum...   \n",
       "2  I bought one of these chargers..the instructio...   \n",
       "3  I was excited to find a book ostensibly about ...   \n",
       "4  I am a big JVC fan, but I do not like this mod...   \n",
       "\n",
       "                                                text  label  \n",
       "0  This model may be ok for sedentary types, but ...      1  \n",
       "1  This is a fast read filled with unexpected hum...      4  \n",
       "2  I bought one of these chargers..the instructio...      2  \n",
       "3  I was excited to find a book ostensibly about ...      2  \n",
       "4  I am a big JVC fan, but I do not like this mod...      2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e49c8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of clean_text in train: 441\n",
      "Max length of review_text in test: 240\n",
      "Max length of review_title in test: 32\n"
     ]
    }
   ],
   "source": [
    "# find the length of longest review_text and review_title of train and test df\n",
    "def find_max_length(df, column):\n",
    "    max_length = df[column].apply(lambda x: len(re.findall(r'\\w+', x))).max()\n",
    "    return max_length\n",
    "max_length_train_clean_text = find_max_length(df_train, 'text')\n",
    "max_length_test_review_text = find_max_length(df_test, 'review_text')\n",
    "max_length_test_review_title = find_max_length(df_test, 'review_title')\n",
    "print(f\"Max length of clean_text in train: {max_length_train_clean_text}\")\n",
    "print(f\"Max length of review_text in test: {max_length_test_review_text}\")\n",
    "print(f\"Max length of review_title in test: {max_length_test_review_title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab24c01",
   "metadata": {},
   "source": [
    "Therefore minimum 512 length of context window is required in our transformer to process each datapoint alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad8c80fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train['label'].value_counts().plot(kind='bar', title='Sentiment Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc9031",
   "metadata": {},
   "source": [
    "The dataset consists of review scores, which is 5 different values form 1-5, 5 is the best. And those are well distributed closely 20% percent for each level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2823d7e",
   "metadata": {},
   "source": [
    "## Try RoBERTa based classification model to classify amazon reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4497b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachith/miniconda3/envs/pytorch/lib/python3.9/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/sachith/miniconda3/envs/pytorch/lib/python3.9/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3028a9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.33.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "configuration = RobertaConfig()\n",
    "print(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cb625ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaModel(\n",
      "  (embeddings): RobertaEmbeddings(\n",
      "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(512, 768, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): RobertaEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): RobertaPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = RobertaModel(configuration)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332a8850",
   "metadata": {},
   "source": [
    "Based on the model configs and the architecture there are few observations.\n",
    "1. the embedding lookup table is 50265x768, which means this accepts 50265 of discrete tokens and embedded them with the 768 vector.\n",
    "2. Max poisitional embedding is 512, therefore it implies that the maximum input tokens is 512 for the model, at least for the existing traiing configs it followed.\n",
    "3. 12 encoder transformer blocks which transform the input embeddings through 12 levels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e766719d",
   "metadata": {},
   "source": [
    "To input the raw data into model, we need to use a tokenizer to represent our natural language inputs by integer. In RoBERTa, they use byte pair encoding which is similar to GPT-2 tokenizer.\n",
    "So after taking the raw text, the tokenizer outputs the sequence of integers, which become thr onput to the Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5737dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachith/miniconda3/envs/pytorch/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "model = RobertaModel.from_pretrained(\"FacebookAI/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9fbad16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 31414, 232, 2], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d94d0",
   "metadata": {},
   "source": [
    "In this assignment, we do not initiate all the parameters (weights) of the models, and train from scratch. Instead we load the pretrained weights from the trained model. -- roberta-base model.\n",
    "The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93a7f6e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    0, 31414,     6,   127,  2335,    16, 11962,     2]]),\n",
       " torch.Size([1, 8]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "inputs.input_ids, inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c076668",
   "metadata": {},
   "source": [
    "We convert the integer list intp pytorch tensor, as the model is built with pytorch and those accepts only tensor inputs.\n",
    "Tensors are similar to multi deimensional arrays, which are optimized for numerical methods and gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edee448c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 124.65M\n"
     ]
    }
   ],
   "source": [
    "model = RobertaModel.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "# n of paramas\n",
    "print(f\"Number of parameters: {model.num_parameters() / 1e6:.2f}M\")\n",
    "outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99b04eb",
   "metadata": {},
   "source": [
    "Model has around 124M parametes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e7218b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last hidden state shape: torch.Size([1, 8, 768])\n",
      "Pooler output shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "last_hidden_state_shape = outputs.last_hidden_state.shape\n",
    "pooler_output_shape = outputs.pooler_output.shape\n",
    "print(f\"Last hidden state shape: {last_hidden_state_shape}\")\n",
    "print(f\"Pooler output shape: {pooler_output_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38a2f2",
   "metadata": {},
   "source": [
    "As we can see the models final layers, the hidden states are one hidden state representation for each input token,\n",
    "and the pooler output is the first hidden state vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703e3de",
   "metadata": {},
   "source": [
    "But, in this assignment we are planning to use differnt pooling stategies like mean/max and finally we may plan to compare the results, if possible the pooler as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb48508",
   "metadata": {},
   "source": [
    "## Building the RoBERTa Sentiment Clasifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3be9d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizerFast\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Define the classifier model\n",
    "class MaxPoolRobertaClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple classifier using RoBERTa with max pooling.\n",
    "    This initiates the RoBERTa model and adds a classifier head on top.\n",
    "    The RoBERTa model is frozen to prevent training.\n",
    "    The classifier head consists of a series of linear layers with ReLU activations and dropout for regularization.\n",
    "    \n",
    "    Args:\n",
    "        num_labels (int): The number of output labels for the classification task.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels=5):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        # Freeze RoBERTa encoder\n",
    "        for param in self.roberta.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Input token IDs.\n",
    "            attention_mask (torch.Tensor): Attention mask to avoid padding tokens.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Logits for each class.\n",
    "        \"\"\"\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state  # [B, L, H]\n",
    "        \n",
    "        # Mask padding tokens for pooling\n",
    "        mask = attention_mask.unsqueeze(-1).expand(hidden_states.size())\n",
    "        masked_hidden = hidden_states.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Max pooling across token dimension\n",
    "        pooled_output = torch.max(masked_hidden, dim=1).values  # [B, H]\n",
    "        \n",
    "        # Feed to classifier head\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a458db6",
   "metadata": {},
   "source": [
    "### Building the data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4aaf3d",
   "metadata": {},
   "source": [
    "To train the model (the prediction head) we need to input the traininig data we have in an efficient way. So that we use pytorch inherent Dataset class and build out loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25728a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset for Amazon reviews\n",
    "class AmazonReviewDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['text']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Return the encoded input and label\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c04ef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader=None, epochs=5, learning_rate=2e-5):\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_accuracy = 0.0\n",
    "    \n",
    "    # Set up learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=1, verbose=True\n",
    "    )\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Progress bar for training\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            print(f\"input_ids shape: {input_ids.shape}\")\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            print(f\"attention_mask shape: {attention_mask.shape}\")\n",
    "            labels = batch['label'].to(device)\n",
    "            print(f\"labels shape: {labels.shape}\")\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Calculate loss - check for nan values and shapes\n",
    "            try:\n",
    "                # Print shape information for debugging\n",
    "                if torch.isnan(outputs).any():\n",
    "                    print(f\"Warning: outputs contain NaN values\")\n",
    "                \n",
    "                # Make sure labels are in the correct range for the model's output classes\n",
    "                if torch.max(labels) >= outputs.size(1):\n",
    "                    print(f\"Error: Labels out of range. Max label: {torch.max(labels).item()}, Output size: {outputs.size(1)}\")\n",
    "                    continue\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Check for NaN loss\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"Warning: NaN loss detected, skipping batch\")\n",
    "                    continue\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Track loss and predictions\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': train_loss / (progress_bar.n + 1)})\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "        print(f'Training Loss: {train_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.4f}')\n",
    "        \n",
    "        # Validation if provided\n",
    "        if val_loader:\n",
    "            val_loss, val_accuracy, val_report = evaluate_model(model, val_loader, criterion, device)\n",
    "            print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}')\n",
    "            print(val_report)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                torch.save(model.state_dict(), 'best_roberta_classifier.pt')\n",
    "                print(f'Model saved with accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a06f5ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_loss = val_loss / len(data_loader)\n",
    "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    val_report = classification_report(all_labels, all_preds)\n",
    "    \n",
    "    return val_loss, val_accuracy, val_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50398bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict(model, text, tokenizer, device):\n",
    "    # Tokenize the text\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "    return preds.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee5aa1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before label adjustment:\n",
      "Train labels: [3 5 4 1 2]\n",
      "Test labels: [1 4 2 3 5]\n",
      "After label adjustment:\n",
      "Train labels: [2 4 3 0 1]\n",
      "Test labels: [0 3 1 2 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachith/miniconda3/envs/pytorch/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Important: Ensure labels start from 0 and are consecutive integers\n",
    "print(\"Before label adjustment:\")\n",
    "print(f\"Train labels: {df_train['label'].unique()}\")\n",
    "print(f\"Test labels: {df_test['label'].unique()}\")\n",
    "\n",
    "# If labels are 1-indexed (1-5) instead of 0-indexed (0-4), adjust them\n",
    "if 0 not in df_train['label'].unique() and 1 in df_train['label'].unique():\n",
    "    df_train['label'] = df_train['label'] - 1\n",
    "    df_test['label'] = df_test['label'] - 1\n",
    "    print(\"After label adjustment:\")\n",
    "    print(f\"Train labels: {df_train['label'].unique()}\")\n",
    "    print(f\"Test labels: {df_test['label'].unique()}\")\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "\n",
    "# Since max text length is 441, we can use a max_length of 512 (standard for RoBERTa)\n",
    "max_length = 512\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AmazonReviewDataset(df_train, tokenizer, max_length)\n",
    "test_dataset = AmazonReviewDataset(df_test, tokenizer, max_length)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16  # Adjust based on your GPU memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ac12b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m MaxPoolRobertaClassifier(num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)  \u001b[38;5;66;03m# 5 classes for Amazon reviews\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust based on your needs\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Evaluate the model on test data\u001b[39;00m\n\u001b[1;32m     14\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 8\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, epochs, learning_rate)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Move model to device\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Loss function and optimizer\u001b[39;00m\n\u001b[1;32m     11\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = MaxPoolRobertaClassifier(num_labels=5)  # 5 classes for Amazon reviews\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    epochs=3,  # Adjust based on your needs\n",
    "    learning_rate=2e-5\n",
    ")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "test_loss, test_accuracy, test_report = evaluate_model(trained_model, test_loader, criterion, device)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}')\n",
    "print(test_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9951acdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
